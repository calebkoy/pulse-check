{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"/kaggle/input/imdb-positive-train-reviews/reviews-pos.csv\n/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n/kaggle/input/imdb-negative-train-reviews/reviews-neg.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pos_df = pd.read_csv('/kaggle/input/imdb-positive-train-reviews/reviews-pos.csv')\ntrain_neg_df = pd.read_csv('/kaggle/input/imdb-negative-train-reviews/reviews-neg.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pos_labels = np.ones(len(train_pos_df.index))\ntrain_neg_labels = np.zeros(len(train_neg_df.index))","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews = pd.concat([train_pos_df, train_neg_df]).to_numpy()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.concatenate((train_pos_labels, train_neg_labels))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nclass ReviewProcessor:\n    def remove_html_tags(self, text):\n        return BeautifulSoup(text).get_text()\n    \n    def remove_non_alpha_or_space_characters(self, text):        \n        return re.sub(r'[^a-zA-Z\\s]', '', text)\n        \n    def remove_short_words(self, text):\n        return re.sub(r'\\b\\w{1,2}\\b', '', text)\n        \n    def remove_stop_words(self, text):\n        pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words(\"english\")) + r')\\b\\s*')\n        return pattern.sub('', text)\n    \n    def lemmatize(self, text):                \n        lemmatizer = WordNetLemmatizer()\n        split_text = text.split()\n        split_text[:] = [lemmatizer.lemmatize(word) for word in split_text]\n        split_text[:] = [lemmatizer.lemmatize(word, pos='v') for word in split_text]\n        return ' '.join(split_text)                \n    \n    def process(self, text):\n        text = self.remove_html_tags(text)\n        text = self.remove_non_alpha_or_space_characters(text)\n        text = self.remove_short_words(text).lower()\n        text = self.remove_stop_words(text)\n        return self.lemmatize(text)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport sys\n\nfrom collections import Counter, defaultdict\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.utils.validation import check_array, check_is_fitted, check_X_y\n\nclass NaiveBayesClassifier(ClassifierMixin, BaseEstimator):\n    def __init__(self, review_processor):\n        self.review_processor = review_processor        \n        \n    def fit(self, X, y):                                                       \n        X, y = check_X_y(X, y, dtype='str')\n        self.classes_, self.y_ = np.unique(y, return_inverse=True)                                                     \n        self.X_ = X.reshape(len(X))        \n        \n        self.__group_data_by_class()\n        self.__compute_log_class_priors()\n        \n        self.vocab_ = set()          \n        self.class_total_word_counts_ = defaultdict(lambda: 0)\n        for c, data in self.grouped_data_.items():                        \n            for index, review in enumerate(data):                                \n                processed_review = self.review_processor.process(review)\n                data[index] = processed_review\n                \n                split_review = processed_review.split()\n                for word in split_review:\n                    self.vocab_.add(word) \n                    \n                word_counts = Counter(split_review)\n                for word, count in word_counts.items():                    \n                    self.class_total_word_counts_[c] += count\n                    \n        self.tf_idf_matrices_ = {}\n        vectorizer = TfidfVectorizer(vocabulary=self.vocab_)\n        for c, data in self.grouped_data_.items():                                                \n            self.tf_idf_matrices_[c] = vectorizer.fit_transform(data).toarray()            \n            \n        self.tf_idf_matrix_feature_names_ = vectorizer.get_feature_names()\n                                    \n        return self\n    \n    def predict(self, X):        \n        check_is_fitted(self)                \n        X = check_array(X, dtype='str')        \n        vocab_size = len(self.vocab_)        \n        predictions = np.empty(len(X))        \n        for index, review in enumerate(X.reshape(len(X))):            \n            predictions[index] = self.__compute_maximum_a_posteriori(review, vocab_size)                    \n        \n        return predictions\n    \n    def __group_data_by_class(self):\n        self.grouped_data_ = {} \n        for index, c in enumerate(self.classes_):\n            self.grouped_data_[c] = self.X_[np.asarray(self.y_ == index).nonzero()]                    \n    \n    def __compute_log_class_priors(self):\n        self.log_class_priors_ = {}\n        number_of_samples = len(self.X_)\n        for c in self.classes_:            \n            self.log_class_priors_[c] = math.log(len(self.grouped_data_[c]) / number_of_samples)        \n            \n        \n    def __compute_maximum_a_posteriori(self, review, vocab_size):\n        max_posterior = -sys.maxsize\n        most_likely_class = -sys.maxsize\n        for c in self.classes_:                \n            posterior = self.log_class_priors_[c]\n            processed_review = self.review_processor.process(review)                \n            word_counts = Counter(processed_review.split())\n            total_words_in_class_reviews = self.class_total_word_counts_[c]  \n            tf_idf_matrix_column_sums = self.tf_idf_matrices_[c].sum(axis=0)\n            for index, word in enumerate(self.vocab_):\n                word_count = word_counts[word]\n                if word_count == 0: \n                    continue\n                tf_idf_matrix_word_column_index = self.tf_idf_matrix_feature_names_.index(word)\n                tf_idf_matrix_column_sum = tf_idf_matrix_column_sums[tf_idf_matrix_word_column_index]\n                laplace_probability = (tf_idf_matrix_column_sum + 1) / (total_words_in_class_reviews + vocab_size)                \n                posterior += (word_count * math.log(laplace_probability))\n            if posterior > max_posterior:\n                max_posterior = posterior\n                most_likely_class = c\n        \n        return most_likely_class        ","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scorer(estimator, X, y):\n    predictions = estimator.predict(X)\n    correct_predictions = (predictions == y)    \n    return correct_predictions.astype(int).mean()","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\n\nnaive_bayes_classifier = NaiveBayesClassifier(ReviewProcessor())\ncv_model = cross_validate(\n    naive_bayes_classifier, reviews, labels, cv=2,\n    scoring=scorer, return_estimator=True, return_train_score=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(sorted(test_scores.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# key = 'train_score'\n# print(type(test_scores[key]))\n# print(test_scores[key])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Tests\n# # =====\n\n# test_review_1 = \"This movie was exceptional!\"\n# test_review_2 = \"I really, really enjoyed 'The Rising'.\"\n# test_review_3 = \"I didn't like this film.\"\n# test_review_4 = \"I'd recommend this show to anyone.\"\n# test_review_5 = \"You'd be silly to spend money on this.\"\n# test_review_6 = \"The show was fine sometimes but mostly boring.\"\n# test_reviews_df = pd.DataFrame({'col': [test_review_1, test_review_2, test_review_3, test_review_4, test_review_5, test_review_6]})\n# test_X = test_reviews_df.to_numpy()\n# test_labels = [1, 1, 0, 1, 0, 0]\n# test_y = np.array(test_labels)\n# # test_clf = NaiveBayesClassifier(ReviewProcessor())\n# # test_clf = test_clf.fit(test_X, test_y)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate\n\n# # Might be simpler to use cross_val_score\n# test_naive_bayes_classifier = NaiveBayesClassifier(ReviewProcessor())\n# test_scores = cross_validate(\n#     test_naive_bayes_classifier, test_X, test_y, scoring=scorer, \n#     cv=RepeatedStratifiedKFold(n_splits=2, n_repeats=2),\n#     return_estimator=True, return_train_score=True\n# )","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(sorted(test_scores.keys()))","execution_count":34,"outputs":[{"output_type":"stream","text":"['estimator', 'fit_time', 'score_time', 'test_score', 'train_score']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# key = 'train_score'\n# print(type(test_scores[key]))\n# print(test_scores[key])","execution_count":38,"outputs":[{"output_type":"stream","text":"<class 'numpy.ndarray'>\n[1. 1. 1. 1.]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_predictions = test_clf.predict(test_X)\n# print(test_predictions)","execution_count":31,"outputs":[{"output_type":"stream","text":"[1. 1. 0. 1. 0. 0.]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # More tests\n# # ==========\n\n# test_test_review_1 = \"The movie is exceptional!\"\n# test_test_review_2 = \"I really liked 'The Rising'.\"\n# test_test_review_3 = \"I didn't like this show.\"\n# test_test_review_4 = \"I'd recommend this show to friends.\"\n# test_test_review_5 = \"You'd be stupid to spend money on this.\"\n# test_test_review_6 = \"The show was okay sometimes but mostly dull.\"\n# test_test_list = [test_test_review_1, test_test_review_2,\n#                   test_test_review_3, test_test_review_4,\n#                   test_test_review_5, test_test_review_6]\n# test_test_df = pd.DataFrame(test_test_list)\n# test_test_X = test_test_df.to_numpy()\n\n# test_test_predictions = test_clf.predict(test_test_X[3:])\n# print(test_test_predictions)","execution_count":32,"outputs":[{"output_type":"stream","text":"[1. 0. 0.]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # More tests\n# # ==========\n\n# test_test_review_1 = \"Far away is a quality piece of showmanship!\"\n# test_test_review_2 = \"I really enjoyed 'The Parachute'.\"\n# test_test_review_3 = \"This one wasn't for me.\"\n# test_test_review_4 = \"I'd tell any person to check this one out.\"\n# test_test_review_5 = \"Unfortunately, this was dry, very dry.\"\n# test_test_review_6 = \"I hate these kinds of movies!\"\n# test_test_list = [test_test_review_1, test_test_review_2,\n#                   test_test_review_3, test_test_review_4,\n#                   test_test_review_5, test_test_review_6]\n# test_test_df = pd.DataFrame(test_test_list)\n# test_test_X = test_test_df.to_numpy()\n\n# test_test_predictions = test_clf.predict(test_test_X)\n# print(test_test_predictions)","execution_count":33,"outputs":[{"output_type":"stream","text":"[0. 1. 0. 0. 0. 1.]\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# # Check if estimator passes sklearn tests\n# from sklearn.utils.estimator_checks import check_estimator\n\n# # pass generate_only=True to run all checks instead of failing at the first error\n# clf = NaiveBayesClassifier(ReviewProcessor())\n# generator = check_estimator(clf, generate_only=True) \n# for estimator, check in generator:\n#     try:\n#         check(estimator)\n#     except Exception as e:\n#         print(e, '\\n')","execution_count":50,"outputs":[{"output_type":"stream","text":"cannot reshape array of size 100 into shape (20,) \n\ncannot reshape array of size 90 into shape (30,) \n\ncannot reshape array of size 42 into shape (21,) \n\ncannot reshape array of size 42 into shape (21,) \n\n\"Complex data not supported\" does not match \"empty vocabulary passed to fit\" \n\ncannot reshape array of size 400 into shape (40,) \n\ncannot reshape array of size 90 into shape (30,) \n\nEstimator doesn't check for NaN and inf in fit. NaiveBayesClassifier(review_processor=<__main__.ReviewProcessor object at 0x7f9187b5e950>) cannot reshape array of size 30 into shape (10,)\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/utils/estimator_checks.py\", line 1463, in check_estimators_nan_inf\n    estimator.fit(X_train, y)\n  File \"<ipython-input-48-2c8ae059d053>\", line 16, in fit\n    self.X_ = X.reshape(len(X))\nValueError: cannot reshape array of size 30 into shape (10,)\ncannot reshape array of size 30 into shape (10,) \n\ncannot reshape array of size 42 into shape (21,) \n\ncannot reshape array of size 90 into shape (30,) \n\ncannot reshape array of size 24 into shape (12,) \n\nClassifier can't train when only one class is present. NaiveBayesClassifier(review_processor=<__main__.ReviewProcessor object at 0x7f9187e51890>) cannot reshape array of size 30 into shape (10,)\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/utils/estimator_checks.py\", line 1767, in check_classifiers_one_label\n    classifier.fit(X_train, y)\n  File \"<ipython-input-48-2c8ae059d053>\", line 16, in fit\n    self.X_ = X.reshape(len(X))\nValueError: cannot reshape array of size 30 into shape (10,)\ncannot reshape array of size 30 into shape (10,) \n\ncannot reshape array of size 40 into shape (20,) \n\ncannot reshape array of size 400 into shape (200,) \n\ncannot reshape array of size 400 into shape (200,) \n\ncannot reshape array of size 400 into shape (200,) \n\n\"Unknown label type: \" does not match \"cannot reshape array of size 6578 into shape (506,)\" \n\ncannot reshape array of size 90 into shape (30,) \n\ncannot reshape array of size 60 into shape (20,) \n\ncannot reshape array of size 60 into shape (20,) \n\ncannot reshape array of size 10 into shape (1,) \n\nempty vocabulary passed to fit \n\ncannot reshape array of size 60 into shape (20,) \n\ncannot reshape array of size 60 into shape (20,) \n\ncannot reshape array of size 160 into shape (80,) \n\ncannot reshape array of size 200 into shape (100,) \n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/estimator_checks.py:3021: FutureWarning: As of scikit-learn 0.23, estimators should have a 'requires_y' tag set to the appropriate value. The default value of the tag is False. An error will be raised from version 0.25 when calling check_estimator() if the tag isn't properly set.\n  warnings.warn(warning_msg, FutureWarning)\n","name":"stderr"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}