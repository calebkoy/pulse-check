{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n/kaggle/input/imdb-positive-train-reviews/reviews-pos.csv\n/kaggle/input/imdb-negative-train-reviews/reviews-neg.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pos_df = pd.read_csv('/kaggle/input/imdb-positive-train-reviews/reviews-pos.csv')\ntrain_neg_df = pd.read_csv('/kaggle/input/imdb-negative-train-reviews/reviews-neg.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pos_labels = np.ones(len(train_pos_df.index))\ntrain_neg_labels = np.zeros(len(train_neg_df.index))","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# print(pd.concat([train_pos_df, train_neg_df]).head().to_numpy().reshape(5,)[0])","execution_count":82,"outputs":[{"output_type":"stream","text":"Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews = pd.concat([train_pos_df, train_neg_df]).to_numpy()","execution_count":83,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.concatenate((train_pos_labels, train_neg_labels))","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nclass ReviewProcessor:\n    def remove_html_tags(self, text):\n        return BeautifulSoup(text).get_text()\n    \n    def remove_non_alpha_or_space_characters(self, text):        \n        return re.sub(r'[^a-zA-Z\\s]', '', text)\n        \n    def remove_short_words(self, text):\n        return re.sub(r'\\b\\w{1,2}\\b', '', text)\n        \n    def remove_stop_words(self, text):\n        pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words(\"english\")) + r')\\b\\s*')\n        return pattern.sub('', text)\n    \n    def lemmatize(self, text):                \n        lemmatizer = WordNetLemmatizer()\n        split_text = text.split()\n        split_text[:] = [lemmatizer.lemmatize(word) for word in split_text]\n        split_text[:] = [lemmatizer.lemmatize(word, pos='v') for word in split_text]\n        return ' '.join(split_text)                \n    \n    def process(self, text):\n        text = self.remove_html_tags(text)\n        text = self.remove_non_alpha_or_space_characters(text)\n        text = self.remove_short_words(text).lower()\n        text = self.remove_stop_words(text)\n        return self.lemmatize(text)","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\nfrom collections import Counter, defaultdict\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.utils.validation import check_array\n\nclass NaiveBayesClassifier(ClassifierMixin, BaseEstimator):\n    def __init__(self, review_processor):\n        self.review_processor = review_processor\n        # TBC        \n        \n    # TODO: refactor\n    def fit(self, X, y):                        \n        # Store the classes seen during fitting                \n        self.classes_, y = np.unique(y, return_inverse=True)                                \n        \n        # Q: Is this necessary? A: TBC\n        #self.X_ = X\n        #self.y_ = y        \n        \n        # TODO: consider extracting to another method\n        # group data by class\n        self.grouped_data_ = {} \n        for index, c in enumerate(self.classes_):\n            self.grouped_data_[c] = X[np.asarray(y == index).nonzero()]                    \n        \n        \n        # TODO: consider extracting to another method\n        # compute log class priors\n        self.log_class_priors_ = {}\n        number_of_samples = X.shape[0]\n        for c in self.classes_:            \n            self.log_class_priors_[c] = math.log(len(self.grouped_data_[c]) / number_of_samples)        \n            \n        # process all reviews\n        for c, data in self.grouped_data_.items():            \n            for index, review in enumerate(data):                \n                # Q: Is the hardcoding of zeros hacky or bad coding style in any way? A: TBC\n                data[index][0] = self.review_processor.process(review[0])\n        \n        # construct the vocab        \n        self.vocab_ = set()\n        for _, data in self.grouped_data_.items():\n            for review in data:\n                split_review = review[0].split()\n                for word in split_review:\n                    self.vocab_.add(word)        \n                \n        \n        # Construct the matrix of TF-IDF features for each class\n        self.document_term_matrices_ = {}\n        for c, data in self.grouped_data_.items():\n            reshaped_data = data.reshape(len(data))\n            self.document_term_matrices_[c] = TfidfVectorizer(vocabulary=vocabs[c]).fit_transform(reshaped_data).toarray()                            \n                            \n        self.class_word_counts_ = {}\n        for c, data in self.grouped_data_.items():\n            self.class_word_counts_[c] = defaultdict(lambda: 0)\n            for review in data:\n                split_review = review[0].split()\n                # TODO: consider creating a counts dictionary using Counter,\n                # instead of incrementing by one each time\n                for word in split_review:\n                    self.class_word_counts_[c][word] += 1\n        \n        \n        #TBC\n        \n        return self\n        \n    def predict(self, X):\n        # TODO: Consider checking if has been fitted. See:\n        # https://scikit-learn.org/stable/developers/develop.html#rolling-your-own-estimator\n        # and\n        # https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/_template.py\n        \n        # Q: is this necessary? See:\n        # https://scikit-learn.org/stable/developers/develop.html#input-validation\n        X = check_array(X)\n        \n        vocab_size = len(self.vocab_)\n        \n        for review in X:\n            # Q: Is it possible to access self.classes_ here?\n            for c in self.classes_:                \n                class_posterior = self.log_class_priors_[c]\n                processed_review = self.review_processor.process(review[0])\n                split_processed_review = processed_review.split() # TODO: consider splitting in previous line\n                word_counts = Counter(split_processed_review)\n                number_of_class_reviews = len(self.grouped_data_[c])\n                # TODO: consider extracting to a method to reduce nesting and improve readability\n                for index, word in enumerate(self.vocab_):\n                    word_count = word_counts[word]\n                    laplace_probability = (self.class_word_counts_[c][word] + 1) / (number_of_class_reviews + vocab_size)\n                    class_posterior += (word_count * math.log(laplace_probability))\n        \n        \n        #TBC\n        \n        #return self.classes_[TBC]","execution_count":94,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = NaiveBayesClassifier(ReviewProcessor())\nclf, document_term_matrix = clf.fit(reviews, labels)","execution_count":95,"outputs":[{"output_type":"stream","text":"(12500, 76014) \n\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Check if estimator passes sklearn tests\nfrom sklearn.utils.estimator_checks import check_estimator\ncheck_estimator(NaiveBayesClassifier()) # pass generate_only=True to run all checks instead of failing at the first error","execution_count":25,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Expected array-like (array or non-string sequence), got None","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-484f8e1ede3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check if estimator passes sklearn tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_checks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcheck_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# pass generate_only=True to run all checks instead of failing at the first error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/estimator_checks.py\u001b[0m in \u001b[0;36mcheck_estimator\u001b[0;34m(Estimator, generate_only)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchecks_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSkipTest\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;31m# the only SkipTest thrown currently results from not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/_testing.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/estimator_checks.py\u001b[0m in \u001b[0;36mcheck_fit_score_takes_y\u001b[0;34m(name, estimator_orig)\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"self\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \"\"\"\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         raise ValueError('Expected array-like (array or non-string sequence), '\n\u001b[0;32m--> 244\u001b[0;31m                          'got %r' % y)\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0msparse_pandas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'SparseSeries'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SparseArray'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected array-like (array or non-string sequence), got None"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scorer(estimator, X, y):\n    #TBC\n    # return single value TBC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"naive_bayes_classifier = NaiveBayesClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RepeatedKFold, cross_validate\n\n# Might be simpler to use cross_val_score\ncv_model = cross_validate(\n    naive_bayes_classifier, reviews, labels, cv=RepeatedKFold(n_splits=5, n_repeats=3),\n    return_estimator=True, n_jobs=-1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load data\n\n# reviews_df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n# reviews_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from bs4 import BeautifulSoup\n\n# # Remove HTML tags from all reviews\n# reviews_df['review'] = [BeautifulSoup(review).get_text() for review in reviews_df['review']]\n# reviews_df.head(20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # remove punctuation and numbers but not spaces\n# import re\n\n# pattern = re.compile(r'[^a-zA-Z\\s]') \n# reviews_df['review'] = [pattern.sub('', review) for review in reviews_df['review']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reviews_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # remove words of length less than 3\n# reviews_df['review'] = [re.sub(r'\\b\\w{1,2}\\b', '', review) for review in reviews_df['review']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reviews_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove stop words\n# from nltk.corpus import stopwords\n\n# pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words(\"english\")) + r')\\b\\s*')\n# reviews_df['review'] = [pattern.sub('', review) for review in reviews_df['review']]\n# reviews_df['review'] = [review.split() for review in reviews_df['review']]\n\n# for review in reviews_df['review']:\n#     review_as_list = review.split()\n#     review_as_list[:] = [pattern.sub('', word) for word ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reviews_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lowercase everything\n# reviews_df['review'] = [review.lower() for review in reviews_df['review']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reviews_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lemmatize\n# from nltk.stem.wordnet import WordNetLemmatizer\n\n# # TODO: figure out if there's a cleaner and/or more efficient way of doing this\n# lemmatizer = WordNetLemmatizer()\n# for index, review in enumerate(reviews_df['review']):\n#     split_review = review.split()\n#     split_review[:] = [lemmatizer.lemmatize(word) for word in split_review]\n#     split_review[:] = [lemmatizer.lemmatize(word, pos='v') for word in split_review]\n#     reviews_df['review'][index] = ' '.join(split_review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reviews_df.head(20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}