# -*- coding: utf-8 -*-
"""process_sentiment140_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10BlVv_fcJUzanGDkzab1zs8Mz01Oh4V7
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

path = "/content/drive/My Drive/Colab Notebooks/data/training.1600000.processed.noemoticon.csv"

entire_sentiment140_df = pd.read_csv(path, header=None, encoding="latin")

import re

def remove_urls(tweet):
  return re.sub(r'www\.\S+|https?://\S+', '', tweet)

def remove_html_character_entities(tweet):
  return re.sub(r'&[a-zA-Z]+;', '', tweet)

def remove_at_mentions(tweet):
  return re.sub(r'@\S+', '', tweet)

def remove_non_alpha_or_space_characters(tweet):        
    return re.sub(r'[^a-zA-Z\s]', '', tweet)

def remove_short_words(tweet):
  return re.sub(r'\b\w{1,2}\b', '', tweet)

def preprocess_tweet(tweet):
  tweet = remove_urls(tweet)
  tweet = remove_html_character_entities(tweet)
  tweet = remove_at_mentions(tweet)
  tweet = remove_non_alpha_or_space_characters(tweet)
  return remove_short_words(tweet)

entire_sentiment140_df[5] = entire_sentiment140_df[5].map(preprocess_tweet)
entire_sentiment140_df

pip install --upgrade scikit-learn

from sklearn.utils import shuffle

entire_sentiment140_df = shuffle(entire_sentiment140_df)
entire_sentiment140_df.reset_index(inplace=True, drop=True)
entire_sentiment140_df

from nltk.corpus import stopwords

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB

text_classifier = Pipeline([
  ('vectorizer', HashingVectorizer(stop_words=stopwords.words('english'),
                                   alternate_sign=False)),
  ('tfidf_transformer', TfidfTransformer()),
  ('classifier', MultinomialNB()),
])

from sklearn.model_selection import GridSearchCV

parameters = {      
  'classifier__alpha': (1e-2, 1e-3), 
}

grid_search_classifier = GridSearchCV(text_classifier, parameters, cv=5, n_jobs=-2)

import time

training_samples = entire_sentiment140_df.iloc[:,-1] 
training_labels = entire_sentiment140_df.iloc[:,0]
tic = time.perf_counter()
grid_search_classifier = grid_search_classifier.fit(training_samples, training_labels)
toc = time.perf_counter()
print(f"Took {toc - tic:0.4f} seconds to fit classifer on data")

print(f"Best score: {grid_search_classifier.best_score_}")

import pickle
with open('test_grid_search_NB_clf_sentiment140.pkl', 'wb') as f:
  pickle.dump(grid_search_classifier, f)

